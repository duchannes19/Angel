{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Dataset Loaded Successfully!\n",
      "                                                 url        type\n",
      "0                                   br-icloud.com.br    phishing\n",
      "1                mp3raid.com/music/krizz_kaliko.html      benign\n",
      "2                    bopsecrets.org/rexroth/cr/1.htm      benign\n",
      "3  http://www.garage-pirenne.be/index.php?option=...  defacement\n",
      "4  http://adventure-nicaragua.net/index.php?optio...  defacement\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file containing the database into Python\n",
    "import pandas as pd\n",
    "\n",
    "# Define the name of the file containing the database\n",
    "filename = 'malicious_phish.csv'\n",
    "\n",
    "# Load the CSV file into Python\n",
    "print('Loading the dataset...')\n",
    "\n",
    "dataset = pd.read_csv(filename)\n",
    "\n",
    "print('Dataset Loaded Successfully!')\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing URLs...\n",
      "Tokenization complete!\n",
      "X_train shape: torch.Size([390714, 512])\n",
      "X_test shape: torch.Size([260477, 512])\n",
      "y_train shape: torch.Size([390714])\n",
      "y_test shape: torch.Size([260477])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Encode the 'type' column into numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "dataset['type'] = label_encoder.fit_transform(dataset['type'])\n",
    "\n",
    "# Split the dataset into features (URLs) and labels (types)\n",
    "X = dataset['url']\n",
    "y = dataset['type']\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the URLs\n",
    "X_tokenized = []\n",
    "\n",
    "print(\"Tokenizing URLs...\")\n",
    "\n",
    "for url in X:\n",
    "    tokens = tokenizer.encode(url, add_special_tokens=True, truncation=True)\n",
    "    X_tokenized.append(tokens)\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "\n",
    "# Pad the tokenized sequences\n",
    "X_padded = pad_sequences(X_tokenized, padding='post', truncating='post', dtype='long', value=0)\n",
    "\n",
    "# Convert the padded sequences and labels into PyTorch tensors\n",
    "X_tensor = torch.tensor(X_padded, dtype=torch.long)\n",
    "y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.4, random_state=42)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "# Create a DataLoader\n",
    "batch_size = 64  \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class URLClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_classes):\n",
    "        super(URLClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(hidden_dim * 256, hidden_dim)  # Adjusted input size for 512 tokens\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.permute(0, 2, 1)  # Reshape for Conv1d\n",
    "        conv_out = self.conv1(embedded)\n",
    "        conv_out = self.relu(conv_out)\n",
    "        pooled = self.pool(conv_out)\n",
    "        pooled = pooled.view(pooled.size(0), -1)\n",
    "        pooled = self.dropout(pooled)\n",
    "        output = self.fc1(pooled)\n",
    "        output = self.relu(output)\n",
    "        output = self.fc2(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "Epoch [1/10], Loss: 0.1403, Accuracy: 0.9545\n",
      "Epoch [2/10], Loss: 0.0748, Accuracy: 0.9760\n",
      "Epoch [3/10], Loss: 0.0578, Accuracy: 0.9809\n",
      "Epoch [4/10], Loss: 0.0489, Accuracy: 0.9837\n",
      "Epoch [5/10], Loss: 0.0423, Accuracy: 0.9860\n",
      "Epoch [6/10], Loss: 0.0384, Accuracy: 0.9872\n",
      "Epoch [7/10], Loss: 0.0344, Accuracy: 0.9884\n",
      "Epoch [8/10], Loss: 0.0315, Accuracy: 0.9894\n",
      "Epoch [9/10], Loss: 0.0288, Accuracy: 0.9902\n",
      "Epoch [10/10], Loss: 0.0264, Accuracy: 0.9911\n",
      "Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define hyperparameters\n",
    "vocab_size = len(tokenizer)  # Size of the vocabulary\n",
    "embedding_dim = 128  # Dimensionality of token embeddings\n",
    "hidden_dim = 64  # Number of output channels for the convolutional layer\n",
    "num_classes = len(label_encoder.classes_)  # Number of classes\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Create an instance of the URLClassifier model\n",
    "model = URLClassifier(vocab_size, embedding_dim, hidden_dim, num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "print('Start training...')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:  \n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Compute training accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Print training statistics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "print('Training completed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start testing...\n",
      "Testing completed!\n",
      "Accuracy on the test set: 0.9845\n"
     ]
    }
   ],
   "source": [
    "# Create a TensorDataset for the test set\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Create a DataLoader for the test set\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define a variable to accumulate the total correct predictions\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "print('Start testing...')\n",
    "# Iterate over batches in the test_loader\n",
    "for inputs, labels in test_loader:  # Assuming you have a DataLoader for test data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "    \n",
    "    # Calculate predictions\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Update total_correct and total_samples\n",
    "    total_correct += (predicted == labels).sum().item()\n",
    "    total_samples += labels.size(0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "\n",
    "print('Testing completed!')\n",
    "print(f'Accuracy on the test set: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'url_classifier.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
