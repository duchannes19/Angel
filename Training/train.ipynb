{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the MATLAB file containing the dataset...\n",
      "The MATLAB file has been loaded.\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'FeatureTypes', 'Day120', 'Day119', 'Day118', 'Day117', 'Day116', 'Day115', 'Day114', 'Day113', 'Day112', 'Day111', 'Day110', 'Day109', 'Day108', 'Day107', 'Day106', 'Day105', 'Day104', 'Day103', 'Day102', 'Day101', 'Day100', 'Day99', 'Day98', 'Day97', 'Day96', 'Day95', 'Day94', 'Day93', 'Day92', 'Day91', 'Day90', 'Day89', 'Day88', 'Day87', 'Day86', 'Day85', 'Day84', 'Day83', 'Day82', 'Day81', 'Day80', 'Day79', 'Day78', 'Day77', 'Day76', 'Day75', 'Day74', 'Day73', 'Day72', 'Day71', 'Day70', 'Day69', 'Day68', 'Day67', 'Day66', 'Day65', 'Day64', 'Day63', 'Day62', 'Day61', 'Day60', 'Day59', 'Day58', 'Day57', 'Day56', 'Day55', 'Day54', 'Day53', 'Day52', 'Day51', 'Day50', 'Day49', 'Day48', 'Day47', 'Day46', 'Day45', 'Day44', 'Day43', 'Day42', 'Day41', 'Day40', 'Day39', 'Day38', 'Day37', 'Day36', 'Day35', 'Day34', 'Day33', 'Day32', 'Day31', 'Day30', 'Day29', 'Day28', 'Day27', 'Day26', 'Day25', 'Day24', 'Day23', 'Day22', 'Day21', 'Day20', 'Day19', 'Day18', 'Day17', 'Day16', 'Day15', 'Day14', 'Day13', 'Day12', 'Day11', 'Day10', 'Day9', 'Day8', 'Day7', 'Day6', 'Day5', 'Day4', 'Day3', 'Day2', 'Day1', 'Day0'])\n"
     ]
    }
   ],
   "source": [
    "# Load MATLAB file containing the dataset into Python\n",
    "import scipy.io\n",
    "import urllib.request\n",
    "import os\n",
    "matlab_file = 'url.mat'\n",
    "\n",
    "# Download the dataset from the following URL: https://www.sysnet.ucsd.edu/projects/url/url.mat\n",
    "\n",
    "if(not os.path.exists(matlab_file)):\n",
    "    download = 'https://www.sysnet.ucsd.edu/projects/url/url.mat'\n",
    "    print('Downloading the dataset from the following URL: ', download )\n",
    "    urllib.request.urlretrieve(download, matlab_file)\n",
    "    print('The dataset has been downloaded successfully.')\n",
    "# Save the dataset in the same directory as the Python script\n",
    "\n",
    "print('Loading the MATLAB file containing the dataset...')\n",
    "\n",
    "data = scipy.io.loadmat(matlab_file)\n",
    "\n",
    "print('The MATLAB file has been loaded.')\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(data['Day2']['labels'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Day0\n",
      "Processing Day1\n",
      "Processing Day2\n",
      "Processing Day3\n",
      "Processing Day4\n",
      "Processing Day5\n",
      "Processing Day6\n",
      "Processing Day7\n",
      "Processing Day8\n",
      "Processing Day9\n",
      "Processing Day10\n",
      "Processing Day11\n",
      "Processing Day12\n",
      "Processing Day13\n",
      "Processing Day14\n",
      "Processing Day15\n",
      "Processing Day16\n",
      "Processing Day17\n",
      "Processing Day18\n",
      "Processing Day19\n",
      "Processing Day20\n",
      "Processing Day21\n",
      "Processing Day22\n",
      "Processing Day23\n",
      "Processing Day24\n",
      "Processing Day25\n",
      "Processing Day26\n",
      "Processing Day27\n",
      "Processing Day28\n",
      "Processing Day29\n",
      "Processing Day30\n",
      "Processing Day31\n",
      "Processing Day32\n",
      "Processing Day33\n",
      "Processing Day34\n",
      "Processing Day35\n",
      "Processing Day36\n",
      "Processing Day37\n",
      "Processing Day38\n",
      "Processing Day39\n",
      "Processing Day40\n",
      "Processing Day41\n",
      "Processing Day42\n",
      "Processing Day43\n",
      "Processing Day44\n",
      "Processing Day45\n",
      "Processing Day46\n",
      "Processing Day47\n",
      "Processing Day48\n",
      "Processing Day49\n",
      "Processing Day50\n",
      "Processing Day51\n",
      "Processing Day52\n",
      "Processing Day53\n",
      "Processing Day54\n",
      "Processing Day55\n",
      "Processing Day56\n",
      "Processing Day57\n",
      "Processing Day58\n",
      "Processing Day59\n",
      "Processing Day60\n",
      "Processing Day61\n",
      "Processing Day62\n",
      "Processing Day63\n",
      "Processing Day64\n",
      "Processing Day65\n",
      "Processing Day66\n",
      "Processing Day67\n",
      "Processing Day68\n",
      "Processing Day69\n",
      "Processing Day70\n",
      "Processing Day71\n",
      "Processing Day72\n",
      "Processing Day73\n",
      "Processing Day74\n",
      "Processing Day75\n",
      "Processing Day76\n",
      "Processing Day77\n",
      "Processing Day78\n",
      "Processing Day79\n",
      "Processing Day80\n",
      "Processing Day81\n",
      "Processing Day82\n",
      "Processing Day83\n",
      "Processing Day84\n",
      "Processing Day85\n",
      "Processing Day86\n",
      "Processing Day87\n",
      "Processing Day88\n",
      "Processing Day89\n",
      "Processing Day90\n",
      "Processing Day91\n",
      "Processing Day92\n",
      "Processing Day93\n",
      "Processing Day94\n",
      "Processing Day95\n",
      "Processing Day96\n",
      "Processing Day97\n",
      "Processing Day98\n",
      "Processing Day99\n",
      "Processing Day100\n",
      "Processing Day101\n",
      "Processing Day102\n",
      "Processing Day103\n",
      "Processing Day104\n",
      "Processing Day105\n",
      "Processing Day106\n",
      "Processing Day107\n",
      "Processing Day108\n",
      "Processing Day109\n",
      "Processing Day110\n",
      "Processing Day111\n",
      "Processing Day112\n",
      "Processing Day113\n",
      "Processing Day114\n",
      "Processing Day115\n",
      "Processing Day116\n",
      "Processing Day117\n",
      "Processing Day118\n",
      "Processing Day119\n",
      "X_all_concatenated shape: torch.Size([240, 1])\n",
      "y_all_concatenated shape: torch.Size([240, 1])\n",
      "X_train shape: torch.Size([192, 1])\n",
      "X_test shape: torch.Size([48, 1])\n",
      "y_train shape: torch.Size([192, 1])\n",
      "y_test shape: torch.Size([48, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract relevant information\n",
    "FeatureTypes = data['FeatureTypes']\n",
    "\n",
    "# Initialize lists to store preprocessed data\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# Iterate over each day\n",
    "for day in range(120):\n",
    "    day_str = \"Day{}\".format(day)\n",
    "    if day_str in data:\n",
    "        print('Processing', day_str)\n",
    "        X_day = data[day_str]['data'][0, 0]\n",
    "        y_day = data[day_str]['labels'][0, 0]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_day = torch.tensor(X_day.shape, dtype=torch.float32)\n",
    "        y_day = torch.tensor(y_day.shape, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_day_normalized = scaler.fit_transform(X_day.numpy().reshape(-1, 1))\n",
    "        X_day = torch.tensor(X_day_normalized, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the labels\n",
    "        y_day_normalized = scaler.fit_transform(y_day.numpy().reshape(-1, 1))\n",
    "        y_day = torch.tensor(y_day_normalized, dtype=torch.float32)\n",
    "        \n",
    "        X_all.append(X_day)\n",
    "        y_all.append(y_day)\n",
    "\n",
    "# Concatenate data from all days\n",
    "X_all_concatenated = torch.cat(X_all, dim=0)\n",
    "y_all_concatenated = torch.cat(y_all, dim=0)\n",
    "\n",
    "print('X_all_concatenated shape:', X_all_concatenated.shape)\n",
    "print('y_all_concatenated shape:', y_all_concatenated.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_concatenated, y_all_concatenated, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: tensor([[-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.]])\n",
      "y_test: tensor([[ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.]])\n",
      "x_train: tensor([[ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.]])\n",
      "x_test: tensor([[-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [-1.],\n",
      "        [ 1.],\n",
      "        [-1.]])\n"
     ]
    }
   ],
   "source": [
    "# Print all labels in the test and training sets\n",
    "print('y_train:', y_train)\n",
    "print('y_test:', y_test)\n",
    "print('x_train:', X_train)\n",
    "print('x_test:', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 1\n",
      "URLClassifier(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network architecture\n",
    "class URLClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(URLClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "num_features = X_train.shape[1]\n",
    "print('Number of features:', num_features)\n",
    "model = URLClassifier(num_features)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 0.9741\n",
      "Epoch [2/100], Loss: 0.9492\n",
      "Epoch [3/100], Loss: 0.9252\n",
      "Epoch [4/100], Loss: 0.9009\n",
      "Epoch [5/100], Loss: 0.8804\n",
      "Epoch [6/100], Loss: 0.8525\n",
      "Epoch [7/100], Loss: 0.8312\n",
      "Epoch [8/100], Loss: 0.8053\n",
      "Epoch [9/100], Loss: 0.7864\n",
      "Epoch [10/100], Loss: 0.7660\n",
      "Epoch [11/100], Loss: 0.7465\n",
      "Epoch [12/100], Loss: 0.7339\n",
      "Epoch [13/100], Loss: 0.7109\n",
      "Epoch [14/100], Loss: 0.6932\n",
      "Epoch [15/100], Loss: 0.6781\n",
      "Epoch [16/100], Loss: 0.6623\n",
      "Epoch [17/100], Loss: 0.6492\n",
      "Epoch [18/100], Loss: 0.6391\n",
      "Epoch [19/100], Loss: 0.6267\n",
      "Epoch [20/100], Loss: 0.6112\n",
      "Epoch [21/100], Loss: 0.6035\n",
      "Epoch [22/100], Loss: 0.6001\n",
      "Epoch [23/100], Loss: 0.5909\n",
      "Epoch [24/100], Loss: 0.5790\n",
      "Epoch [25/100], Loss: 0.5733\n",
      "Epoch [26/100], Loss: 0.5665\n",
      "Epoch [27/100], Loss: 0.5609\n",
      "Epoch [28/100], Loss: 0.5582\n",
      "Epoch [29/100], Loss: 0.5508\n",
      "Epoch [30/100], Loss: 0.5471\n",
      "Epoch [31/100], Loss: 0.5423\n",
      "Epoch [32/100], Loss: 0.5395\n",
      "Epoch [33/100], Loss: 0.5376\n",
      "Epoch [34/100], Loss: 0.5335\n",
      "Epoch [35/100], Loss: 0.5302\n",
      "Epoch [36/100], Loss: 0.5303\n",
      "Epoch [37/100], Loss: 0.5271\n",
      "Epoch [38/100], Loss: 0.5248\n",
      "Epoch [39/100], Loss: 0.5226\n",
      "Epoch [40/100], Loss: 0.5185\n",
      "Epoch [41/100], Loss: 0.5207\n",
      "Epoch [42/100], Loss: 0.5184\n",
      "Epoch [43/100], Loss: 0.5176\n",
      "Epoch [44/100], Loss: 0.5160\n",
      "Epoch [45/100], Loss: 0.5152\n",
      "Epoch [46/100], Loss: 0.5135\n",
      "Epoch [47/100], Loss: 0.5134\n",
      "Epoch [48/100], Loss: 0.5135\n",
      "Epoch [49/100], Loss: 0.5121\n",
      "Epoch [50/100], Loss: 0.5118\n",
      "Epoch [51/100], Loss: 0.5115\n",
      "Epoch [52/100], Loss: 0.5106\n",
      "Epoch [53/100], Loss: 0.5110\n",
      "Epoch [54/100], Loss: 0.5100\n",
      "Epoch [55/100], Loss: 0.5100\n",
      "Epoch [56/100], Loss: 0.5094\n",
      "Epoch [57/100], Loss: 0.5089\n",
      "Epoch [58/100], Loss: 0.5091\n",
      "Epoch [59/100], Loss: 0.5086\n",
      "Epoch [60/100], Loss: 0.5086\n",
      "Epoch [61/100], Loss: 0.5077\n",
      "Epoch [62/100], Loss: 0.5078\n",
      "Epoch [63/100], Loss: 0.5072\n",
      "Epoch [64/100], Loss: 0.5073\n",
      "Epoch [65/100], Loss: 0.5073\n",
      "Epoch [66/100], Loss: 0.5077\n",
      "Epoch [67/100], Loss: 0.5066\n",
      "Epoch [68/100], Loss: 0.5067\n",
      "Epoch [69/100], Loss: 0.5067\n",
      "Epoch [70/100], Loss: 0.5066\n",
      "Epoch [71/100], Loss: 0.5064\n",
      "Epoch [72/100], Loss: 0.5065\n",
      "Epoch [73/100], Loss: 0.5066\n",
      "Epoch [74/100], Loss: 0.5062\n",
      "Epoch [75/100], Loss: 0.5063\n",
      "Epoch [76/100], Loss: 0.5062\n",
      "Epoch [77/100], Loss: 0.5059\n",
      "Epoch [78/100], Loss: 0.5059\n",
      "Epoch [79/100], Loss: 0.5059\n",
      "Epoch [80/100], Loss: 0.5060\n",
      "Epoch [81/100], Loss: 0.5054\n",
      "Epoch [82/100], Loss: 0.5055\n",
      "Epoch [83/100], Loss: 0.5053\n",
      "Epoch [84/100], Loss: 0.5055\n",
      "Epoch [85/100], Loss: 0.5053\n",
      "Epoch [86/100], Loss: 0.5052\n",
      "Epoch [87/100], Loss: 0.5057\n",
      "Epoch [88/100], Loss: 0.5054\n",
      "Epoch [89/100], Loss: 0.5054\n",
      "Epoch [90/100], Loss: 0.5051\n",
      "Epoch [91/100], Loss: 0.5051\n",
      "Epoch [92/100], Loss: 0.5052\n",
      "Epoch [93/100], Loss: 0.5050\n",
      "Epoch [94/100], Loss: 0.5049\n",
      "Epoch [95/100], Loss: 0.5050\n",
      "Epoch [96/100], Loss: 0.5051\n",
      "Epoch [97/100], Loss: 0.5052\n",
      "Epoch [98/100], Loss: 0.5048\n",
      "Epoch [99/100], Loss: 0.5051\n",
      "Epoch [100/100], Loss: 0.5049\n",
      "Test Loss: 0.5044\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming you have defined your model and device earlier\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "# Move data and labels to the same device \n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "\n",
    "    # Ensure the output has the same shape as y_train\n",
    "    outputs = outputs  # Remove the extra dimension\n",
    "\n",
    "    loss = criterion(outputs, y_train)  # Remove the extra dimension from y_train as well\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_outputs = test_outputs.squeeze()  # Remove the extra dimension\n",
    "    test_loss = criterion(test_outputs, y_test.squeeze())  # Remove the extra dimension from y_test as well\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2500\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "threshold = 0.5\n",
    "predictions = (test_outputs > threshold).float()\n",
    "accuracy = (predictions == y_test).float().mean()\n",
    "print(f'Accuracy: {accuracy.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
