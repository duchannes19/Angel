{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MATLAB file has been loaded.\n",
      "dict_keys(['__header__', '__version__', '__globals__', 'FeatureTypes', 'Day120', 'Day119', 'Day118', 'Day117', 'Day116', 'Day115', 'Day114', 'Day113', 'Day112', 'Day111', 'Day110', 'Day109', 'Day108', 'Day107', 'Day106', 'Day105', 'Day104', 'Day103', 'Day102', 'Day101', 'Day100', 'Day99', 'Day98', 'Day97', 'Day96', 'Day95', 'Day94', 'Day93', 'Day92', 'Day91', 'Day90', 'Day89', 'Day88', 'Day87', 'Day86', 'Day85', 'Day84', 'Day83', 'Day82', 'Day81', 'Day80', 'Day79', 'Day78', 'Day77', 'Day76', 'Day75', 'Day74', 'Day73', 'Day72', 'Day71', 'Day70', 'Day69', 'Day68', 'Day67', 'Day66', 'Day65', 'Day64', 'Day63', 'Day62', 'Day61', 'Day60', 'Day59', 'Day58', 'Day57', 'Day56', 'Day55', 'Day54', 'Day53', 'Day52', 'Day51', 'Day50', 'Day49', 'Day48', 'Day47', 'Day46', 'Day45', 'Day44', 'Day43', 'Day42', 'Day41', 'Day40', 'Day39', 'Day38', 'Day37', 'Day36', 'Day35', 'Day34', 'Day33', 'Day32', 'Day31', 'Day30', 'Day29', 'Day28', 'Day27', 'Day26', 'Day25', 'Day24', 'Day23', 'Day22', 'Day21', 'Day20', 'Day19', 'Day18', 'Day17', 'Day16', 'Day15', 'Day14', 'Day13', 'Day12', 'Day11', 'Day10', 'Day9', 'Day8', 'Day7', 'Day6', 'Day5', 'Day4', 'Day3', 'Day2', 'Day1', 'Day0'])\n"
     ]
    }
   ],
   "source": [
    "# Load MATLAB file containing the dataset into Python\n",
    "import scipy.io\n",
    "import urllib.request\n",
    "import os\n",
    "matlab_file = 'url.mat'\n",
    "\n",
    "# Download the dataset from the following URL: https://www.sysnet.ucsd.edu/projects/url/url.mat\n",
    "\n",
    "if(not os.path.exists(matlab_file)):\n",
    "    download = 'https://www.sysnet.ucsd.edu/projects/url/url.mat'\n",
    "    print('Downloading the dataset from the following URL: ', download )\n",
    "    urllib.request.urlretrieve(download, matlab_file)\n",
    "    print('The dataset has been downloaded successfully.')\n",
    "# Save the dataset in the same directory as the Python script\n",
    "\n",
    "print('Loading the MATLAB file containing the dataset...')\n",
    "\n",
    "data = scipy.io.loadmat(matlab_file)\n",
    "\n",
    "print('The MATLAB file has been loaded.')\n",
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_all_concatenated shape: torch.Size([240, 1])\n",
      "y_all_concatenated shape: torch.Size([240])\n",
      "X_train shape: torch.Size([192, 1])\n",
      "X_test shape: torch.Size([48, 1])\n",
      "y_train shape: torch.Size([192])\n",
      "y_test shape: torch.Size([48])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Extract relevant information\n",
    "FeatureTypes = data['FeatureTypes']\n",
    "\n",
    "# Initialize lists to store preprocessed data\n",
    "X_all = []\n",
    "y_all = []\n",
    "\n",
    "# Iterate over each day\n",
    "for day in range(120):\n",
    "    day_str = \"Day{}\".format(day)\n",
    "    if day_str in data:\n",
    "        X_day = data[day_str]['data'][0, 0]\n",
    "        y_day = data[day_str]['labels'][0, 0]\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        X_day = torch.tensor(X_day.shape, dtype=torch.float32)\n",
    "        y_day = torch.tensor(y_day.shape, dtype=torch.float32)\n",
    "        \n",
    "        # Normalize the data\n",
    "        scaler = StandardScaler()\n",
    "        X_day_normalized = scaler.fit_transform(X_day.numpy().reshape(-1, 1))\n",
    "        X_day = torch.tensor(X_day_normalized, dtype=torch.float32)\n",
    "        \n",
    "        X_all.append(X_day)\n",
    "        y_all.append(y_day)\n",
    "\n",
    "# Concatenate data from all days\n",
    "X_all_concatenated = torch.cat(X_all, dim=0)\n",
    "y_all_concatenated = torch.cat(y_all, dim=0)\n",
    "\n",
    "print('X_all_concatenated shape:', X_all_concatenated.shape)\n",
    "print('y_all_concatenated shape:', y_all_concatenated.shape)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all_concatenated, y_all_concatenated, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train: tensor([1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04, 1.6000e+04,\n",
      "        2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04,\n",
      "        1.0000e+00, 1.3000e+02, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 2.0000e+04, 2.0000e+04, 2.0000e+04, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00,\n",
      "        1.0000e+00, 1.0000e+00, 2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04])\n",
      "y_test: tensor([2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00, 2.0000e+04, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
      "        2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04, 2.0000e+04, 1.0000e+00,\n",
      "        2.0000e+04, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 2.0000e+04,\n",
      "        1.0000e+00, 2.0000e+04, 2.0000e+04, 2.0000e+04, 1.0000e+00, 2.0000e+04])\n"
     ]
    }
   ],
   "source": [
    "# Print all labels in the test and training sets\n",
    "print('y_train:', y_train)\n",
    "print('y_test:', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLClassifier(\n",
      "  (fc1): Linear(in_features=1, out_features=128, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the neural network architecture\n",
    "class URLClassifier(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(URLClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the neural network\n",
    "num_features = X_train.shape[1]\n",
    "model = URLClassifier(num_features)\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_train\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Backward pass and optimization\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mURLClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(x)\n\u001b[0;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[1;32mc:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andri\\miniconda3\\envs\\ml\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Move data and labels to the same device\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.unsqueeze(1))\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Print the loss for the current epoch\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    test_loss = criterion(test_outputs, y_test.unsqueeze(1))\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
